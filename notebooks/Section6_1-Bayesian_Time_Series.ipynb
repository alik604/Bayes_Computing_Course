{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time series analysis is one of the more important applications in data science, given the importance placed on forecasting and prediction. At the same time, it presents a new set of challenges to analysts:\n",
    "\n",
    "* many of the typical statistical assumptions do not apply\n",
    "* time series data are typically sparser than static data\n",
    "* model validation is more difficult\n",
    "\n",
    "At its simplest, time series data are sequences of observations, where each observation can be related to previous observations. \n",
    "\n",
    "> *Time series* is a series of data points indexed (or listed or graphed) in time order.\n",
    "\n",
    "This implies a lack of independence among the observations; specifically, the order of the observations is important, and must be taken into account for any analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pymc3 as pm\n",
    "import theano.tensor as tt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import arviz as az\n",
    "import pandas as pd\n",
    "from scipy.linalg import cholesky\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "RANDOM_SEED = 20090425"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Statistical Models\n",
    "\n",
    "Often we don't need a full mechanistic model, but rather seek to build simple models which capture the time series behaviour of the data. These may be used to provide an adequate basis for forecasting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Pancreatitis prevalence\n",
    "\n",
    "Pancreatitis is the inflammation of the pancreas, most commonly caused by alcohol or gallstones. A systematic review in 2010 yielded several age-specific point estimates of incidence for the disease accross Europe. Here are the data for Finland."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(pm.get_data('pancreatitis.csv'))\n",
    "data = data[data.area=='FIN']\n",
    "\n",
    "age = data['age'] = np.array(data.age_start + data.age_end) / 2\n",
    "rate = data.value = data.value * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data.age, data.value, '.')\n",
    "plt.xlabel('age')\n",
    "plt.ylabel('incidence');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try to model this series (which is not really a time series) as a Gaussian random walk. A random walk is a sequence of variables whereby the next value in the sequence is the current value plus a random value, drawn independently from some distribution $P$:\n",
    "\n",
    "$$y_t = y_{t+1} + \\epsilon_{t+1} $$\n",
    "\n",
    "$$\\epsilon_t \\sim P$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of a Gaussian random walk, the distribution $P$ is a normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vector of latent incidence is given a prior distribution by `GaussianRandomWalk`. As its name suggests GaussianRandomWalk is a vector valued distribution where the values of the vector form a random normal walk of length n, as specified by the `shape` argument. The scale of the innovations of the random walk, `sigma`, is specified in terms of the standard deviation of the normally distributed innovations and can be a scalar or vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the log-likelihood of `GaussianRandomWalk` in PyMC3:\n",
    "\n",
    "```python\n",
    "def logp(self, x):\n",
    "\n",
    "        x_im1 = x[:-1]\n",
    "        x_i = x[1:]\n",
    "\n",
    "        innov_like = Normal.dist(mu=x_im1 + self.mu, sigma=self.sigma).logp(x_i)\n",
    "        return self.init.logp(x[0]) + tt.sum(innov_like)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nknots = 9\n",
    "knots = np.linspace(data.age.min(), data.age.max(), nknots)\n",
    "\n",
    "with pm.Model() as ghme_model:\n",
    "    \n",
    "    coeff_sd = pm.HalfCauchy('coeff_sd', 5)\n",
    "    y = pm.GaussianRandomWalk('y', sigma=coeff_sd, shape=nknots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to generalize from the knots of the model to other points across the range of the data--most notably, to the observed values--we require an interpolation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate(x0, y0, x):\n",
    "    x = np.array(x)\n",
    "\n",
    "    idx = np.searchsorted(x0, x)\n",
    "    dl = np.array(x - x0[idx - 1])\n",
    "    dr = np.array(x0[idx] - x)\n",
    "    d = dl + dr\n",
    "    wl = dr / d\n",
    "\n",
    "    return wl * y0[idx - 1] + (1 - wl) * y0[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ghme_model:\n",
    "    p = interpolate(knots, y, age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All that remains is to specify the likelihood, which in this case can be specifiewd as Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ghme_model:\n",
    "\n",
    "    sd = pm.HalfCauchy('sd', 5)\n",
    "    vals = pm.Normal('vals', p, sigma=sd, observed=rate.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ghme_model:\n",
    "    trace = pm.sample(1000, tune=1000, cores=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data.age, data.value, '.')\n",
    "plt.plot(knots, trace[y][::5].T, color='r', alpha=.01)\n",
    "\n",
    "plt.ylim(0, rate.max());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace, var_names=['sd', 'coeff_sd']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_forest(trace, var_names=['y']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate Gaussian Random Walk\n",
    "\n",
    "The pancreatitis dataset consists of a set of countries, for which we may wish to jointly estimate disease incidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(pm.get_data('pancreatitis.csv'))\n",
    "countries = ['CYP', 'DNK', 'ESP', 'FIN', 'GBR', 'ISL']\n",
    "data = data[data.area.isin(countries)]\n",
    "\n",
    "age = data['age'] = np.array(data.age_start + data.age_end) / 2\n",
    "rate = data.value = data.value * 1000\n",
    "group, countries = pd.factorize(data.area, order=countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncountries = len(countries)\n",
    "plt.figure(figsize=(12,6))\n",
    "\n",
    "for i, country in enumerate(countries):\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    plt.title(country)\n",
    "    d = data[data.area == country]\n",
    "    plt.plot(d.age, d.value, '.')\n",
    "\n",
    "    plt.ylim(0, rate.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To jointly model sets of (correlated) time series, we can use the `MvGaussianRandomWalk` class, which models a set of time series with Gaussian innovations. This requires generalizing the `sigma` scale parameter for the random walk to a covariance matrix in the multivariate case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the [inverse-Wishart distribution](https://en.wikipedia.org/wiki/Inverse-Wishart_distribution) is the conjugate prior for the covariance matrix of a multivariate normal distribution, it is [not very well-suited](https://github.com/pymc-devs/pymc3/issues/538#issuecomment-94153586) to modern Bayesian computational methods.  For this reason, the [LKJ prior](http://www.sciencedirect.com/science/article/pii/S0047259X09000876) is recommended when modeling the covariance matrix of a multivariate normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sampling distribution for the multivariate normal model is $\\mathbf{x} \\sim N(\\mu, \\Sigma)$, where $\\Sigma$ is the covariance matrix of the sampling distribution, with $\\Sigma_{ij} = \\textrm{Cov}(x_i, x_j)$.  The density of this distribution is\n",
    "\n",
    "$$f(\\mathbf{x}\\ |\\ \\mu, \\Sigma^{-1}) = (2 \\pi)^{-\\frac{k}{2}} |\\Sigma|^{-\\frac{1}{2}} \\exp\\left(-\\frac{1}{2} (\\mathbf{x} - \\mu)^{\\top} \\Sigma^{-1} (\\mathbf{x} - \\mu)\\right).$$\n",
    "\n",
    "The LKJ distribution provides a prior on the correlation matrix, $\\mathbf{C} = \\textrm{Corr}(x_i, x_j)$, which, combined with priors on the standard deviations of each component, [induces](http://www3.stat.sinica.edu.tw/statistica/oldpdf/A10n416.pdf) a prior on the covariance matrix, $\\Sigma$.  Since inverting $\\Sigma$ is numerically unstable and inefficient, it is computationally advantageous to use the [Cholesky decompositon](https://en.wikipedia.org/wiki/Cholesky_decomposition) of $\\Sigma$, $\\Sigma = \\mathbf{L} \\mathbf{L}^{\\top}$, where $\\mathbf{L}$ is a lower-triangular matrix.  This decompositon allows computation of the term $(\\mathbf{x} - \\mu)^{\\top} \\Sigma^{-1} (\\mathbf{x} - \\mu)$ using back-substitution, which is more numerically stable and efficient than direct matrix inversion.\n",
    "\n",
    "PyMC3 supports LKJ priors for the Cholesky decomposition of the covariance matrix via the [LKJCholeskyCov](../api/distributions/multivariate.rst) distribution.  This distribution has parameters `n` and `sd_dist`, which are the dimension of the observations, $\\mathbf{x}$, and the PyMC3 distribution of the component standard deviations, repsectively.  It also has a hyperparamter `eta`, which controls the amount of correlation between components of $\\mathbf{x}$.  The LKJ distribution has the density $f(\\mathbf{C}\\ |\\ \\eta) \\propto |\\mathbf{C}|^{\\eta - 1}$, so $\\eta = 1$ leads to a uniform distribution on correlation matrices, while the magnitude of correlations between components decreases as $\\eta \\to \\infty$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as ghme_model:\n",
    "    \n",
    "    packed_L = pm.LKJCholeskyCov('packed_L', n=ncountries,   \n",
    "                             eta=2., sd_dist=pm.HalfCauchy.dist(2.5))\n",
    "    L = pm.expand_packed_triangular(ncountries, packed_L)\n",
    "\n",
    "    y = pm.MvGaussianRandomWalk('y', chol=L, shape=(nknots, ncountries))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of the model is identical to the univariate case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate(x0, y0, x, group):\n",
    "    x = np.array(x)\n",
    "    group = np.array(group)\n",
    "\n",
    "    idx = np.searchsorted(x0, x)\n",
    "    dl = np.array(x - x0[idx - 1])\n",
    "    dr = np.array(x0[idx] - x)\n",
    "    d = dl + dr\n",
    "    wl = dr / d\n",
    "\n",
    "    return wl * y0[idx - 1, group] + (1 - wl) * y0[idx, group]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ghme_model:\n",
    "\n",
    "    p = interpolate(knots, y, age, group)\n",
    "\n",
    "    sd = pm.HalfCauchy('sd', 5)\n",
    "\n",
    "    vals = pm.Normal('vals', p, sigma=sd, observed=rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ghme_model:\n",
    "    trace = pm.sample(1000, tune=1000, cores=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "\n",
    "for i, country in enumerate(countries):\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    plt.title(country)\n",
    "\n",
    "    d = data[data.area == country]\n",
    "    plt.plot(d.age, d.value, '.')\n",
    "    plt.plot(knots, trace[y][::5, :, i].T, color='r', alpha=.01)\n",
    "\n",
    "    plt.ylim(0, rate.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_forest(trace, var_names=['packed_L']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Stochastic Volatility\n",
    "\n",
    "Asset prices have time-varying volatility (variance of day over day `returns`). In some periods, returns are highly variable, while in others very stable. Stochastic volatility models model this with a latent volatility variable, modeled as a stochastic process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = pd.read_csv('../data/SP500.csv', index_col='date')['change']\n",
    "returns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns.plot(figsize=(10,5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct an appropriate model for this data using a Gaussian random walk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as stochastic_vol_model:\n",
    "    step_size = pm.Exponential('step_size', 10)\n",
    "    volatility = pm.GaussianRandomWalk('volatility', sigma=step_size, shape=len(returns))\n",
    "    nu = pm.Exponential('nu', 0.1)\n",
    "    y = pm.StudentT('y', \n",
    "                    nu=nu, \n",
    "                    lam=np.exp(-2*volatility), \n",
    "                    observed=returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.model_to_graphviz(stochastic_vol_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with stochastic_vol_model:\n",
    "    trace = pm.sample(2000, tune=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 4))\n",
    "\n",
    "y_vals = np.exp(trace['volatility'])[::5].T\n",
    "x_vals = np.vstack([returns.index for _ in y_vals.T]).T.astype(np.datetime64)\n",
    "\n",
    "plt.plot(x_vals, y_vals, 'k', alpha=0.002)\n",
    "ax.set_xlim(x_vals.min(), x_vals.max())\n",
    "ax.set_ylim(bottom=0)\n",
    "ax.set(title='Estimated volatility over time', xlabel='Date', ylabel='Volatility');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoregressive Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gaussian random walk is a special case of an autoregressive model, which is specified by:\n",
    "\n",
    "$$y_t = \\rho y_{t-1} + \\epsilon_t,$$\n",
    "   \n",
    "where $\\epsilon_t \\overset{iid}{\\sim} {\\cal N}(0,1)$. In the case of the GRW, the parameter $\\rho$ is fixed to 1; consequentially, the random increments alone drive the evolution of the state (hence the name, \"random walk\"). \n",
    "\n",
    "The form above is also a specific subclass of autoregressive model, the first-order autoregressive, or AR(1), process. This is a Markovian model because the next state is a function only of the current state. \n",
    "\n",
    "A yet more general form of autoregressive model is the nth-order autoregressive process, AR(n):\n",
    "\n",
    "$$y_t = \\rho_` y_{t-1} + \\rho_2 y_{t-2} + \\ldots + \\rho_n y_{t-n} + \\epsilon_t$$\n",
    "\n",
    "Let's start with estimating an AR(1) model using PyMC3. First, let's generate some simulated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed=42)\n",
    "\n",
    "T = 100\n",
    "y = np.zeros(T)\n",
    "\n",
    "for i in range(1,T):\n",
    "    y[i] = 0.95 * y[i-1] + np.random.normal()\n",
    "\n",
    "plt.plot(y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use a Bayesian approach, we must choose a prior for the coefficient $\\rho$. We will use a Gaussian prior $\\rho \\sim {\\cal N}(0,\\tau^2)$.\n",
    "This results in a posterior distribution of $\\rho$ of the form:\n",
    "\n",
    "$$\n",
    " \\rho |Y^T \\sim {\\cal N}( \\tilde{\\rho}_T, \\tilde{V}_T),\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "        \\tilde{\\rho}_T &=& \\left( \\sum_{t=1}^T y_{t-1}^2 + \\tau^{-2} \\right)^{-1} \\sum_{t=1}^T y_{t}y_{t-1} \\\\\n",
    "        \\tilde{V}_T      &=& \\left( \\sum_{t=1}^T y_{t-1}^2 + \\tau^{-2} \\right)^{-1}\n",
    "\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as ar1:\n",
    "    \n",
    "    ρ = pm.Normal('ρ', mu=0, sigma=1.0)\n",
    "    ts = pm.AR('ts', ρ, sigma=1.0, observed=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ar1:    \n",
    "    trace = pm.sample(1000, tune=2000, cores=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_p = ((y[:-1]**2).sum() + 1**-2)**-1 * np.dot(y[:-1],y[1:])\n",
    "V_p =  ((y[:-1]**2).sum() + 1**-2)**-1\n",
    "\n",
    "print('Mean: {:5.3f} (exact = {:5.3f})'.format(trace['ρ'].mean(), mu_p))\n",
    "print('Std: {:5.3f} (exact = {:5.3f})'.format(trace['ρ'].std(), np.sqrt(V_p)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extension to AR(p)\n",
    "We can instead estimate an AR(2) model using pyMC3.\n",
    "\n",
    "$$\n",
    " y_t = \\rho_1 y_{t-1} + \\rho_2 y_{t-2} + \\epsilon_t.\n",
    "$$\n",
    "\n",
    "The `AR` distribution infers the order of the process by size the of `rho` argmument passed to `AR`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as ar2:\n",
    "    ρ = pm.Normal('ρ', mu=0, sigma=1, shape=2)\n",
    "    likelihood = pm.AR('likelihood', ρ, sigma=1.0, observed=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ar2:\n",
    "    trace = pm.sample(1000, tune=2000, cores=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the estimate of $\\rho_1$ is close to zero, which is understandable since the data was simulated from an AR(1) model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a moving average: ARMA\n",
    "\n",
    "More complex time series models are typically achieved by adding other components to the basic autoregressive model. \n",
    "\n",
    "A common approach is to use a **moving average**; a moving average model uses past forecast errors in a regression-like model:\n",
    "\n",
    "$$y_{t}=c+\\varepsilon_{t}+\\theta_{1} \\varepsilon_{t-1}+\\theta_{2} \\varepsilon_{t-2}+\\cdots+\\theta_{q} \\varepsilon_{t-q}$$\n",
    "\n",
    "Notice that the observation $y_t$ can be viewed as a weighted moving average of the past several errors. So a first-order MA process is:\n",
    "\n",
    "$$y_{t}=c+\\varepsilon_{t}+\\theta_{1} \\varepsilon_{t-1}$$\n",
    "\n",
    "This is homologous to smoothing, but a moving average model is used for forecasting future values, whereas smoothing is used for estimating the trend-cycle of past values.\n",
    "\n",
    "The motivation for the MA model is that we can explain shocks in the error process directly by fitting a model to the error terms.\n",
    "\n",
    "> As a general rule, a low order AR process will give rise to a high order MA process and the low order MA process will give rise to a high order AR process.\n",
    "> $$x_{t}=\\lambda x_{t-1}+\\varepsilon_{t}, \\quad \\lambda<1$$\n",
    "> by successively lagging this equation and substituting out the lagged value of x we may rewrite this as, \n",
    "> $$x_{t}=\\sum_{j=1}^{\\infty} \\lambda^{j} \\varepsilon_{t-j} \\quad \\text { where } \\lambda^{\\infty} x_{t-\\infty} \\rightarrow 0$$\n",
    "> So the first order AR process has been recast as an infinite order MA one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An AR(p) and a MA(q) process can be combined to yield an autoregressive moving average (ARMA) model as follows:\n",
    "\n",
    "$$y_{t}=c+\\phi_{1} y_{t-1}+\\cdots+\\phi_{p} y_{t-p}+\\theta_{1} \\varepsilon_{t-1}+\\cdots+\\theta_{q} \\varepsilon_{t-q}+\\varepsilon_{t}$$\n",
    "\n",
    "Why would we want such similar components in the same model? The AR process accounts for **trends** in the stochastic process, while the MA component will soak up **unexpected events** in the time series.\n",
    "\n",
    "A common data transformation that is applied to non-stationary time series to render them stationary is **differencing**. The differenced series is the change between consecutive observations in the original series, and can be written as\n",
    "\n",
    "$$y_{t}^{\\prime}=y_{t}-y_{t-1}$$\n",
    " \n",
    "The differenced series will have only T-1 values, since it is not possible to calculate a difference for the first observation. Applying the ARMA to differnced data yeilds an autoregressive **integrated** moving average (ARIMA) model.\n",
    "\n",
    "$$y_{t}^{\\prime}=c+\\phi_{1} y_{t-1}^{\\prime}+\\cdots+\\phi_{p} y_{t-p}^{\\prime}+\\theta_{1} \\varepsilon_{t-1}+\\cdots+\\theta_{q} \\varepsilon_{t-q}+\\varepsilon_{t}$$\n",
    "\n",
    "For our purposes, we will stick with the ARMA model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing and ARMA model in PyMC3 is trickier than for the AR(n) process. It involves generating variables in a loop, which PyMC3 is not very good at, due to the underlying Theano architecture. For this, we need to add some Theano code to our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to fit an ARMA model to a sample dataset. We will use a common time series dataset, which is just a summary of monthly totals of international airline passengers between 1949 to 1960."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Go ahead and run this model while you read--it takes a few minutes to fit.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_passengers = pd.read_csv('../data/AirPassengers.csv', parse_dates=[0]).set_index('Month')\n",
    "air_passengers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_passengers.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first have some minor data processing to do: to make our code simpler, we should turn the pandas `Series` data structure into a Theano `shared` variable. These are hybrid symbolic and non-symbolic variables whose value may be shared between multiple functions. Shared variables can be used in symbolic expressions in Theano, but they also have an internal value that defines the value taken by this symbolic variable in all the functions that use it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from theano import shared\n",
    "\n",
    "y = shared(air_passengers.values.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start, as always, by declaring our priors, which here consist of:\n",
    "\n",
    "- observation error standard deviation: $\\sigma$\n",
    "- initial stsate: $\\mu$\n",
    "- moving average coefficient: $\\theta$\n",
    "- autoregression coefficient: $\\rho$\n",
    "\n",
    "For simplicity, we will model a ARMA(1, 1) process, so first order for both the moving average and autoregression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as arma_model:\n",
    "    \n",
    "    σ = pm.HalfNormal('σ', 5.)\n",
    "    μ = pm.Normal('μ', 100., sigma=10.)\n",
    "    θ = pm.Normal('θ', 0., sigma=1.)\n",
    "    ρ = pm.Normal('ρ', 0., sigma=2.)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tricky part comes with calculating the sequence of states. We cannot simply use a python `for` loop; instead, we need to write a Theano `scan` function. \n",
    "\n",
    "The `scan` function provides the basic functionality needed to do loops in Theano. There are three things that we need to handle: \n",
    "\n",
    "1. the initial value assigned to the result\n",
    "2. the accumulation of results\n",
    "3. the non-sequence values required by the calculation in the loop \n",
    "\n",
    "Unchanging variables are passed to scan as `non_sequences`, initialization occurs in `outputs_info`, and the accumulation happens automatically.\n",
    "\n",
    "Scan returns a tuple containing our result (`err`) and a dictionary of updates, which we do not need so it is assigned to the throwaway variable `_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from theano import scan\n",
    "\n",
    "with arma_model:\n",
    "\n",
    "    err0 = y[0] - (μ + ρ * μ)\n",
    "\n",
    "    def calc_next(last_y, this_y, err, μ, ρ, θ):\n",
    "        nu_t = μ + ρ * last_y + θ * err\n",
    "        return this_y - nu_t\n",
    "\n",
    "    err, _ = scan(fn=calc_next,\n",
    "                  sequences=dict(input=y, taps=[-1, 0]),\n",
    "                  outputs_info=[err0],\n",
    "                  non_sequences=[μ, ρ, θ])\n",
    "    \n",
    "    likelhood = pm.Normal('likelihood', 0, sigma=σ, observed=err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that, for convenience, we are modeling the residuals in our likelhiood function, hence the likelihood has a zero-mean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with arma_model:\n",
    "    trace = pm.sample(draws=1000,\n",
    "                      tune=1000,\n",
    "                      target_accept=.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Structural Time Series Models\n",
    "\n",
    "**Go ahead and run this model while you read--it takes a few minutes to fit.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach to time series modeling involves the use of **state-space models**, which has its origins in control engineering. For example, in navigation systems one requires continuous updating of a user's position, based on noisy data. This is analogous to what time series models try to do: make inferences about a *latent state*, based on a sequence of data. In this context, they are known as **structural time series models**. They are generally more transparent than ARIMA-type models because it is not based on autoregression or moving averages, which are not intuitive, and they are flexible and modular, making them widely-applicable to a variety of settings.\n",
    "\n",
    "The modularity of structural time series models is their key feature. Specifically, they are comprised of an **observation equation** that specifies how the data are related to the unobserved state, and a **state dynamics equation**, which describes how the latent state evolves over time.\n",
    "\n",
    "### Observation equation\n",
    "\n",
    "$$y_t = \\mu_t + \\epsilon_t$$\n",
    "\n",
    "The observation equation relates the observed data with the concurrent value of the unobserved state $\\mu_t$. This is typially assumed to be Gaussian, but need not be:\n",
    "\n",
    "$$\\epsilon_t \\sim N(0, \\sigma_{\\epsilon})$$\n",
    "\n",
    "### State dynamics equation\n",
    "\n",
    "$$\\mu_{t+1} = \\mu_t + \\beta X_t + S_t + \\eta_t$$\n",
    "\n",
    "The state dynamics equation models the temporal dynamics of the baseline mean $\\mu_t$, and is sometimes called the **unobserved trend**, since we never observe $\\mu$ (though it is typically what we want to infer about). Thus, we are assuming that the state is somehow changing over time.\n",
    "\n",
    "This regession component optionally models the influence of a set of predictor variables $X_t$, as well as a seasonality component $S_t$ on an observed time series of data $\\{y_t\\}$.\n",
    "\n",
    "Analogous to the observation error, we typically assume the system errors $\\eta_t$ are drawn from some random, zero-centered distribution:\n",
    "\n",
    "$$\\eta_t \\sim N(0, \\sigma_{\\eta})$$\n",
    "\n",
    "Additionally, we assume $\\epsilon_t$ and $\\eta_t$ are uncorrelated.\n",
    "\n",
    "![state space model](images/state_space.png)\n",
    "\n",
    "This modular structure allows the uncertainty in consituent components to be handled separately. Yet, using a Bayesian approach for inference allows all components to be estimated **simultaneously**. All estimated quantities will have posterior distributions that can be used for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Snowshoe hare population dynamics\n",
    "\n",
    "We can use structural time series modeling to create a phenominalogical model of Showshoe hare (*Lepus americanus*) data. We will use a dataset consisting of 7 years of regular counts as our time series, modeling the latent population and the observation process simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hare_data = pd.read_excel('../data/4_Controls Hare Live trap data Kluane.xlsx', \n",
    "              sheet_name='Silver Old 10x10 overview', usecols=['date', '# Checks', '# Indiv'],\n",
    "             parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hare_data.plot('date', '# Indiv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = hare_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as hare_model:\n",
    "    \n",
    "    σ_ar = pm.HalfNormal('σ_ar', 1)\n",
    "    \n",
    "    ρ = pm.Normal('ρ', 1, sigma=1)\n",
    "    μ = pm.AR('μ', ρ, sigma=σ_ar, shape=T)\n",
    "        \n",
    "    α = pm.HalfNormal('α', 1)\n",
    "    like = pm.NegativeBinomial('like', mu=pm.math.exp(μ), alpha=α, observed=hare_data['# Indiv'].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.model_to_graphviz(hare_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with hare_model:\n",
    "    trace = pm.sample(1000, tune=1000, cores=2, random_seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace, var_names=['σ_ar', 'α', 'ρ']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "(pd.DataFrame(np.exp(trace['μ'].mean(0)), index=hare_data.date)\n",
    "      .plot(legend=False, color='Green', ax=ax))\n",
    "(pd.DataFrame(np.quantile(np.exp(trace['μ']), [0.05, 0.95], axis=0).T, index=hare_data.date)\n",
    "      .plot(legend=False, color='Green', ax=ax, alpha=0.5))\n",
    "# hare_data.plot('date', '# Indiv', ax=ax, alpha=0.3, color='grey');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an idea about how our model performs, we can sample from the posterior predictive distribution, using `sample_posterior_predictive`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with hare_model:\n",
    "    pred_trace = pm.sample_posterior_predictive(trace, samples=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hare_data.index, pred_trace['like'].T, color='g', alpha=0.01);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explicit seasonality model\n",
    "\n",
    "**Go ahead and run this model while you read--it takes a few minutes to fit.**\n",
    "\n",
    "The basic structured time series models can be problematic to fit partly because it is trying to do a lot. For example, is obvious (particularly in the airline data) that there is a periodic effect in the data-generatimg model, and we are trying to make the autoregressive component account for this as well as the (linear?) trend in the mean that is independent of the seasonality.\n",
    "\n",
    "Thus, it is wise to add an explicit seasonality component to the model, which will allow the AR(1) component to look after the trend. There are several commonly used state-component models to capture seasonality. For example:\n",
    "\n",
    "$$\\begin{split} & y_t = \\tau_t +\\epsilon_t, \\\\ & \\tau_{t+d} = - \\sum_{i=0}^{s-2}\\tau_{t-i\\times d} + \\eta_{\\tau, t}, \\end{split}$$\n",
    "\n",
    "where $s$ is the number of seasons and $d$ is the seasonal duration (number of time periods in each season, often set to 1). The model can be thought of as a regression on $s$ dummy variables representing $s$ seasons and $\\tau_{t}$ denotes their joint contribution to the observed response $y_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean of $\\tau_{t+d}$ is such that the total seasonal effect is zero when summed over $s$ seasons\n",
    "\n",
    "$$E(\\tau_{t+d}+\\sum_{i=0}^{s-2}\\tau_{t-i\\times d}) = 0$$\n",
    "\n",
    "It can be helpful to display this component in matrix form:\n",
    "\n",
    "$$\\begin{split} & y_t = [1\\quad 0 \\quad \\cdots\\quad 0]\\left[\\begin{matrix}\\tau_{t}\\\\\\tau_{t-d}\\\\ \\vdots\\\\ \\tau_{t-(s-2)d}\\end{matrix}\\right] +\\epsilon_t, \\\\ & \\left[\\begin{matrix}\\tau_{t+d}\\\\\\tau_t\\\\\\tau_{t-d}\\\\ \\vdots\\\\ \\tau_{t-(s-4)d}\\\\ \\tau_{t-(s-3)d}\\end{matrix}\\right] = \\left[\\begin{matrix} -1 & - 1 & \\cdots & -1 & -1 \\\\ 1 & 0 & \\cdots &0& 0\\\\ 0 & 1 & \\cdots & 0 &0 \\\\ \\vdots &\\vdots &\\vdots &\\vdots &\\vdots &\\\\ 0 & 0 & \\cdots & 1 & 0 \\\\ 0 & 0 & \\cdots & 0 & 0 \\\\ \\end{matrix}\\right] \\left[\\begin{matrix}\\tau_{t}\\\\\\tau_{t-d}\\\\\\tau_{t-2d}\\\\\\vdots \\\\ \\tau_{t-(s-3)d}\\\\ \\tau_{t-(s-2)d}\\end{matrix}\\right] + \\left[\\begin{matrix}1\\\\0\\\\0\\\\ \\vdots\\\\ 0\\\\ 0\\end{matrix}\\right]\\eta_{\\tau, t} \\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the easy part--essentially copying the AR(1) component from the previous model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = air_passengers.shape[0]\n",
    "\n",
    "with pm.Model() as ts_model_seasonal:\n",
    "    \n",
    "    # Standard deviations\n",
    "    σ_η = pm.HalfNormal('σ_η', 25)\n",
    "    z_η = pm.Normal('z_η', shape=S)\n",
    "    η = pm.Deterministic('η', z_η * σ_η)\n",
    "    \n",
    "    # State dynamics\n",
    "    σ_μ = pm.HalfNormal('σ_μ', 25)\n",
    "    ρ = pm.Normal('ρ', 0, sigma=1)\n",
    "    μ = pm.AR('μ', ρ, sigma=σ_μ, shape=T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement the seasonality term, we create $S-1$ variables, and concatenate them with their negative sum, which enforces a sum-to-one constraint. Since the data are monthly, there are 12 \"seasons\" (and, as it happens, 12 years for a total of 144 observations). Thus, we repeat the 12 month variables 12 times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_years = int(T/S)\n",
    "\n",
    "with ts_model_seasonal:\n",
    "    \n",
    "    # Seasonality\n",
    "    σ_τ = pm.HalfNormal('σ_τ', 25)\n",
    "    z_τ = pm.Normal('z+τ', shape=S-1)\n",
    "    τ = pm.Deterministic('τ', z_τ * σ_τ)\n",
    "    s = tt.concatenate([[-1*tt.sum(τ)], τ]*n_years) + tt.repeat(η, S)\n",
    "    \n",
    "    # Likelihood\n",
    "    σ_y = pm.HalfNormal('σ_y', 25)\n",
    "    like = pm.Normal('like', μ + s, sigma=σ_y, observed=air_passengers.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.model_to_graphviz(ts_model_seasonal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ts_model_seasonal:\n",
    "    trace = pm.sample(1000, tune=2000, cores=2, random_seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace, var_names=['σ_η', 'σ_μ','σ_y', 'ρ']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_forest(trace, var_names=['τ']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ts_model_seasonal:\n",
    "    pred_trace = pm.sample_posterior_predictive(trace, samples=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pred_trace['like'].T, color='g', alpha=0.01);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Electricity demand\n",
    "\n",
    "Below is a 6-week dataset of electricity demand in the state of Victoria in Austrialia. Along with this data is an important covariate, temperature. Try modeling the time series with and without the covariate information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from electricity_demand_data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = sns.color_palette()\n",
    "c1, c2 = colors[0], colors[1]\n",
    "\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "ax = fig.add_subplot(2, 1, 1)\n",
    "ax.plot(demand_dates[:-num_forecast_steps],\n",
    "        demand[:-num_forecast_steps], lw=2, label=\"training data\")\n",
    "ax.set_title(\"Demand\")\n",
    "ax.set_ylabel(\"Hourly demand (GW)\")\n",
    "\n",
    "ax = fig.add_subplot(2, 1, 2)\n",
    "\n",
    "ax.plot(demand_dates[:-num_forecast_steps],\n",
    "        temperature[:-num_forecast_steps], lw=2, label=\"training data\", c=c2)\n",
    "ax.set_ylabel(\"Temperature (deg C)\")\n",
    "ax.set_title(\"Temperature\")\n",
    "ax.xaxis.set_major_locator(demand_loc)\n",
    "ax.xaxis.set_major_formatter(demand_fmt)\n",
    "fig.suptitle(\"Electricity Demand in Victoria, Australia (2014)\",\n",
    "             fontsize=15)\n",
    "fig.autofmt_xdate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# References\n",
    "\n",
    "Lyle Broemeling [Bayesian Analysis of Time Series](https://www.amazon.com/Bayesian-Analysis-Time-Lyle-Broemeling/dp/1138591521)\n",
    "\n",
    "[Quantopian Lecture Series](https://www.quantopian.com/lectures)\n",
    "\n",
    "Tingting Yu,  [Structural Time Series Models](http://oliviayu.github.io/post/2019-03-21-bsts/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
